# ğŸ›’ Retail ETL Data Pipeline Project

## ğŸ“Œ Overview

This project demonstrates an **ETL (Extract, Transform, Load)** pipeline built using **PySpark** for processing retail data. It ingests raw CSV files, transforms them into a **star schema**, and stores the results in **PostgreSQL** and **Parquet** format. The transformed data is then ready to be visualized using tools like **Power BI**.

---

## ğŸ—‚ï¸ Dataset Sources

The pipeline processes three retail-related datasets:
- `sales data-set.csv` â€“ Weekly sales data by store and department
- `Features data set.csv` â€“ Store-specific promotional and economic data
- `stores data-set.csv` â€“ Store metadata including type and size

---

## âš™ï¸ Technologies Used

- **PySpark** â€“ Data processing and transformation
- **PostgreSQL** â€“ Relational database used for staging and star schema layers
- **Parquet** â€“ Columnar format for optimized storage
- **Power BI** â€“ Business intelligence tool for dashboarding (optional)
- **Hive/HDFS (optional)** â€“ Extendable for big data storage and analytics
- **pandas** â€“ Used for final Parquet conversion due to Windows compatibility issues

  ---

## ğŸ—ï¸ Project Workflow

```text
            Raw CSV Files
                   â†“
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚   Extract (PySpark) â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
   PostgreSQL (retail_stg schema - staging layer)
                   â†“
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  Transform (PySpark) â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
    PostgreSQL (retail_dw schema - star schema)
                   â†“
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚   Load (Parquet)    â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
      Power BI (Visualization)

---

ğŸ“ Repository Structure
retail-etl-pipeline/
â”œâ”€â”€ extract.py                 # Ingests and cleans CSVs, loads into staging
â”œâ”€â”€ transform.py              # Transforms staging data to star schema
â”œâ”€â”€ load.py                   # Converts PostgreSQL tables to Parquet
â”œâ”€â”€ utils.py                  # Reusable Spark & DB config + data cleaning
â”œâ”€â”€ /DirectoryTo/Dataset 1/   # Raw input CSV files
â”œâ”€â”€ /DirectoryTo/ETL/         # Output Parquet files
â”œâ”€â”€ README.md                 # Project documentation


ğŸš€ How to Run the Pipeline
âš™ï¸ 1. Configure Database Connection
Edit utils.py and replace with your PostgreSQL credentials:
jdbc_url = "jdbc:postgresql://<HOST>:5432/<DB_NAME>"
properties = {
    "user": "<USERNAME>",
    "password": "<PASSWORD>",
    "driver": "org.postgresql.Driver"
}

ğŸ“¥ 2. Extract and Load to Staging (retail_stg)
python extract.py

Loads raw CSVs

Cleans nulls and bad formats

Writes cleaned data to retail_stg schema in PostgreSQL


ğŸ”„ 3. Transform into Star Schema (retail_dw)

python transform.py

Joins and transforms staging tables

Builds dimension and fact tables

Loads them into retail_dw schema in PostgreSQL

ğŸ’¾ 4. Convert Tables to Parquet Format

python load.py

Reads tables from retail_dw

Converts to Parquet using Pandas

Stores files in /DirectoryTo/ETL/

ğŸ“Š Visualize with Power BI
Open Power BI Desktop.

Connect to:

PostgreSQL â†’ Load tables from retail_dw for live querying.

Parquet files â†’ Load locally exported Parquet data for offline analysis.

Build visualizations using dimensions like store, dept, and date, and measures like weekly_sales


ğŸ“¬ Contact
For questions or improvements, feel free to reach out via LinkedIn or raise an issue in the repository.

